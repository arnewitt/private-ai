services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"

  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    volumes:
      - ./config/litellm.yaml:/app/config.yaml
    command: --config /app/config.yaml --detailed_debug
    env_file: 
      - .env
    ports:
      - "4000:4000"
  
  ollama:
    image: docker.io/ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ${HOME}/.ollama:/root/.ollama # reuse models from host
    # If model not available on host location ${HOME}/.ollama, download model in running container:
    # 1) docker exec -it <container_name> ollama run <model_name>   or
    # 2) docker exec -it <container_name> ollama pull <model_name>
    environment:
      - OLLAMA_MAX_LOADED_MODELS=2 # idea: one LLM and one embedding model
